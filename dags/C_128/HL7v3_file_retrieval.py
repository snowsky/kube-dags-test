from airflow import DAG
from airflow.decorators import task
from airflow.models.param import Param
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.exceptions import AirflowFailException
from functools import partial
import math 
import logging
# If we want to utilise ProcessPoolExecutor we need to set
# AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER = true
from concurrent.futures import as_completed, ThreadPoolExecutor as PoolExecutor
from C_128.utils import execute_query

DEFAULT_DEST_FILES_DIRECTORY = '/data/biakonzasftp/C-128/archive/HL7v3In' #ask eric where he wants the files to go
DEFAULT_AWS_FOLDER = 'HL7v3In' 
DEFAULT_MAX_POOL_WORKERS = 5
DEFAULT_MAX_TASKS = 200
DEFAULT_PAGE_SIZE = 1000 # Splits aws list_keys into batches
PARALLEL_TASK_LIMIT = 5  # Change this to large number of prod to remove parallel task limit
#DEFAULT_AWS_TAG = 'CPProcessed'
DEFAULT_DB_CONN_ID = 'qa-az1-sqlw3-airflowconnection' 


class BucketDetails:
    def __init__(self, aws_conn_id, s3_hook_kwargs):
        self.aws_conn_id = aws_conn_id
        self.s3_hook_kwargs = s3_hook_kwargs


AWS_BUCKETS = {'konzaandssigrouppipelines':
                   BucketDetails(aws_conn_id='konzaandssigrouppipelines',
                                 s3_hook_kwargs={}),
               'com-ssigroup-insight-attribution-data':
                   BucketDetails(aws_conn_id='konzaandssigrouppipelines',
                                 s3_hook_kwargs={'encrypt': True, 'acl_policy':'bucket-owner-full-control'}),
               'mial-test-bucket':
                   BucketDetails(aws_conn_id='test_aws',
                                 s3_hook_kwargs={})}
default_args = {
    'owner': 'airflow',
}
with DAG(
    dag_id='HL7v3_file_retrieval_AA',
    default_args=default_args,
    schedule=None,
    tags=['C-128'],
    concurrency=PARALLEL_TASK_LIMIT,
    params={
        "output_files_dir_path": Param(DEFAULT_DEST_FILES_DIRECTORY, type="string"),
        "aws_bucket": Param('konzaandssigrouppipelines', type="string"),
        "aws_folder": Param(DEFAULT_AWS_FOLDER, type="string"),
        #"aws_tag": Param(DEFAULT_AWS_TAG, type="string"),
        "max_pool_workers": Param(DEFAULT_MAX_POOL_WORKERS, type="integer", minimum=0),
        "max_mapped_tasks": Param(DEFAULT_MAX_TASKS, type="integer", minimum=0),
        "page_size": Param(DEFAULT_PAGE_SIZE, type="integer", minimum=0)
    },
) as dag:
    def _split_list_into_batches(target_list,  max_tasks):
        if target_list:
            chunk_size = math.ceil(len(target_list) / max_tasks)
            batches = [target_list[i:i + chunk_size] for i in range(0, len(target_list), chunk_size)]
        else:
            batches = []
        return batches

    def _archive_results_from_futures(future_file_dict):
        results, exceptions = _get_results_from_futures(future_file_dict)
        batch_values = ', '.join([f"('{value}')" for value in results])
        execute_query(f"INSERT INTO archive.processed_files (file_key) VALUES {batch_values}", DEFAULT_DB_CONN_ID)
        if exceptions:
            raise AirflowFailException(f'exceptions raised: {exceptions}')

    def _get_results_from_futures(future_file_dict):
        results = []
        exceptions = []
        for future in as_completed(future_file_dict):
            file = future_file_dict[future]
            try:
                results.append(future.result())
            except Exception as e:
                exceptions.append(str(f'{file}: {str(e)})'))
        return results, exceptions

    @task(task_id='retrieve_file_from_s3')
    def retrieve_file_from_s3_task(batch_id, params: dict):
        result = execute_query(f"SELECT file_key FROM temp.file_keys_filtered WHERE batch_id={batch_id};",
                               DEFAULT_DB_CONN_ID)
        key_list = [r[0] for r in result]
        bucket_name = params["aws_bucket"]
        bucket = AWS_BUCKETS[bucket_name]
        max_workers = params['max_pool_workers']
        with PoolExecutor(max_workers=max_workers) as executor:
            future_file_dict = {executor.submit(partial(_retrieve_file_from_s3, params, bucket.aws_conn_id,
                                                        bucket_name, bucket.s3_hook_kwargs), f): f for f in
                                key_list}
            _archive_results_from_futures(future_file_dict)

    def _retrieve_file_from_s3(params, aws_conn_id, aws_bucket_name, s3_hook_kwargs, aws_key):
        s3_hook = S3Hook(aws_conn_id=aws_conn_id)
        s3_hook.download_file(
            key=aws_key,
            bucket_name=aws_bucket_name,
            local_path=params["output_files_dir_path"],
            preserve_file_name=True,
            use_autogenerated_subdir=False,
            **s3_hook_kwargs
        )
        return aws_key

    @task
    def get_file_keys_task(params):
        bucket_name = params["aws_bucket"]
        aws_folder = params["aws_folder"]
        bucket = AWS_BUCKETS[bucket_name]
        s3_hook = S3Hook(aws_conn_id=bucket.aws_conn_id)
        key_list = s3_hook.list_keys(bucket_name=bucket_name, prefix=aws_folder, page_size=params["page_size"])
        batch_key_list = _split_list_into_batches(key_list, params["max_mapped_tasks"])
        batch_values = ', '.join([f"('{b}', '{ind}')" for ind, a in enumerate(batch_key_list) for b in a])
        execute_query(f"INSERT INTO temp.file_keys (file_key, batch_id) VALUES {batch_values}", DEFAULT_DB_CONN_ID)
        return [i for i in range(0, len(key_list))]

    @task
    def filter_file_keys_task(batch_id, params):
        archive_result = execute_query(f"SELECT file_key FROM archive.processed_files;", DEFAULT_DB_CONN_ID)
        database_key_list = [r[0] for r in archive_result]

        aws_result = execute_query(f"SELECT file_key FROM temp.file_keys WHERE batch_id={batch_id};", DEFAULT_DB_CONN_ID)
        aws_key_list = [r[0] for r in aws_result]

        bucket_name = params["aws_bucket"]
        bucket = AWS_BUCKETS[bucket_name]
        s3_hook = S3Hook(aws_conn_id=bucket.aws_conn_id)
        conn = s3_hook.get_conn()
        #diff_list = [k for k in aws_key_list if k not in database_key_list and
                     #any(tag.get("Key") == params["aws_tag"] for tag in conn.get_object_tagging(Bucket=bucket_name, Key=k).get('TagSet', []))]
        diff_list = [k for k in aws_key_list if k not in database_key_list or
                     any(tag.get("Key") == params["aws_tag"] for tag in conn.get_object_tagging(Bucket=bucket_name, Key=k).get('TagSet', []))]
        if diff_list:
            batch_values = ', '.join([f"('{key}')" for key in diff_list])
            query = f"INSERT INTO temp.file_keys_filtered (file_key) VALUES {batch_values}"
            execute_query(query,DEFAULT_DB_CONN_ID)
            #execute_query(f"INSERT INTO temp.file_keys_filtered (file_key) VALUES {batch_values}", DEFAULT_DB_CONN_ID)
            #logging.info(f'Skipping {file_key} as it starts with “airflow”')
            logging.info("here is the query(codeline140 : " + query)
        return len(diff_list)
        

    @task
    def prep_temp_tables_task():
        query = f"""
                DROP TABLE IF EXISTS temp.file_keys;
                CREATE TABLE temp.file_keys (file_key varchar(255), batch_id int);
                CREATE INDEX batch_id_index ON temp.file_keys (batch_id);
                DROP TABLE IF EXISTS temp.file_keys_filtered;
                CREATE TABLE temp.file_keys_filtered (file_key varchar(255), batch_id int);
                CREATE INDEX batch_id_filtered_index ON temp.file_keys_filtered (batch_id);
                """
        execute_query(query, DEFAULT_DB_CONN_ID)

    @task
    def split_files_into_batches_task(diff_list_lengths, params):
        diff_list_length = sum(diff_list_lengths)
        if diff_list_length:
            chunk_size = math.ceil(diff_list_length / params["max_mapped_tasks"])
            query = f"""
                    UPDATE temp.file_keys_filtered t
                    JOIN (
                        SELECT file_key,
                               ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS row_num
                        FROM temp.file_keys_filtered
                    ) n ON t.file_key = n.file_key
                    SET t.batch_id = ((n.row_num - 1) / {chunk_size});
                    """
            execute_query(query, DEFAULT_DB_CONN_ID)
            return [i for i in range(0, math.ceil(diff_list_length / chunk_size))]
        return []

    prep_temp_tables = prep_temp_tables_task()
    get_file_keys = get_file_keys_task()
    filter_file_keys = filter_file_keys_task.expand(batch_id=get_file_keys)
    split_files_into_batches = split_files_into_batches_task(filter_file_keys)
    retrieve_file_from_s3 = retrieve_file_from_s3_task.expand(batch_id=split_files_into_batches)

    prep_temp_tables >> get_file_keys >> filter_file_keys >> split_files_into_batches >> retrieve_file_from_s3
